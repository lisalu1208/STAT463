---
title: 'Assignment 2 STAT 315-463:  Multivariable Statistical Methods and Applications'
author: 'Lisa Lu 31088272'
header-includes:
   - \usepackage{bm}
   - \usepackage{amsmath}
output: 
  pdf_document:
    number_sections: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
\vspace{-8 mm}
**Due date: Friday 24 March 2023**  
\vspace{-7 mm}

- Your assignment needs to show the R code you used, and your well discussed answers to  the questions. 
- Submit your assignments on Learn.
\vspace{- 2 mm}

## Background

In the dataset, \texttt{USJudgeRatings.csv}, you are presented with ratings of State Judges on the Superior Court on 12 variables provided by 43 Lawyers in 1977.\vspace{-5 mm}

\begin{center}
\begin{tabular}{l|l||l|l||l|l} \hline
CONT &	Number of contacts of lawyer with judge &
INTG &	Judicial integrity &
DMNR &	Demeanour \\
DILG &	Diligence &
CFMG &	Case flow managing &
DECI &	Prompt decisions \\
PREP &	Preparation for trial &
FAMI &	Familiarity with law &
ORAL &	Sound oral rulings \\
WRIT &	Sound written rulings &
PHYS &	Physical ability &
RTEN &	Worthy of retention \\ \hline
\end{tabular}\end{center}

```{r}
# Read in the data
dataset <- read.csv("USJudgeRatings.csv")
```

## Principal Component Analysis of the Rating Data.

Perform a PCA on the standardised ratings. Note, you will need to standardise the ratings yourself. Then answer the following questions.

```{r}
# Standardise the ratings
dataset_std <- data.frame(scale(dataset[,-1]))
```


1. How many principal components do you believe should be retained. Justify your answer by looking at the variation in the data explained by each component.

```{r}
# Perform Pricipal Component Analysis on the standardised dataset
pca <- prcomp(dataset_std)
# Use the scree plot
screeplot(pca)
```
From the scree plot, we can clearly see that the first two principal components can capture most of the variation from the data. Third and fourth components still capture a few variation but not as much. From the fifth onward, the values are very close to 0. Therefore, I think no more than the first four principal components should be retained.  

```{r}
# Get the percentage of variance explained by each component
summary(pca)
```

It is shown that the first principal component (PC1) explains 84.4% of variation, with PC2 accounts for 9.2%, PC3 for 2.8%, and PC4 for 2.1%. The first 3 PCs explain 96.4% of the variation and the first 4 PCs explain 98.5% of the variation. So from this result, I think even just take the first 3 PCs should be enough to cover a good percentage of variation.     

2. In your own words, describe what you believe the first principal component is measuring.

```{r}
pca$rotation[, 1:2]
```
Rotations, or loadings, specify the weight that each variable contribute to the component. In the result above, we can see that all the variables, except CONT, have negative values. Among those, the component has largest negative associations with ORAL, WRIT. It seems like the first PC is measuring the judges' work skills and qualities. 

3. What do you think the second principal component represents?

The second principal component is the linear combination of variables that covers as much of the remaining variation as possible. From the result above, we can see that it has a large negative association with CONT. Therefore, this component is primarily measuring the judges' interpersonal communication with the lawyers. This component has 0 correlation with the first PC.


4. You are told *Judicial Integrity* and *Demeanour* are particularly important traits, and should be given 5 times the weight of the other variables. Re-run the Principal Component Analysis such that Integrity and Demeanour is given 5 times the weight of all other variables. What impact does this have?
```{r}
# Apply weights to INTG and DMNR
dataset_weighed <- dataset_std
dataset_weighed$INTG <- dataset_std$INTG * 5
dataset_weighed$DMNR <- dataset_std$DMNR * 5

# Run the principal component analysis on the weighed dataset
pca1 <- prcomp(dataset_weighed)
summary(pca1)
pca1$rotation[, 1:3]
```

From the results above, we can see that with the increasing of the weights to variables, it increases the associations between the components and the variables and the associations between the components with other variables decrease. 

## Factor Analysis for the Rating Data
 
Perform Factor Analysis on the standardised Ratings Data. 
 
1. What happens when you try to fit a 3 and a 4 factor solution with no rotation. Hint: For the three factor solution, you may need to add \texttt{control=list(nstart=100)} as an additional argument in the \texttt{factanal} function.

```{r}
factor3 <- factanal(dataset_std, factors = 3, scores = "regression", 
                    rotation = "none", control=list(nstart=100))
factor3
factor4 <- factanal(dataset_std, factors = 4, scores = "regression", 
                    rotation = "none")
factor4
```
From the results of loadings above, it seems like there is not much difference between a 3 and a 4 factor without a rotation. 

2. Which variables are grouped by the first two factors? (e.g. threshold $|\text{loading}| \geq 0.25$)
```{r}
biplot(factor3$scores, factor3$loadings)
```
From the plot, we can see that INTG and DMNR are grouped together, CONT is in its own group, and all the rest of the variables seems to be in one group. 
```{r}
factor3$correlation
```
From the correlation matrix, we can also see that CONT does not have much correlation with other variables. INTG has high correlation with DMNR (0.96). All the other variables have very high correlations with one another. 

3. Compare the factor loadings for the first two factors to the first two principal components of the standardised data found in the previous section. Comment on any similarities and/or differences.
```{r}
# Comparison of factor loadings from FA and PCA
cbind(factor3$loadings[,1:2], pca$rotation[, 1:2])
```
CONT always have the opposite association in the first factor and the first PC and the second factor and second PC have more association with this variable. 

```{r}
#Plots of FA vs PCA Loadings
par(mfrow=c(1,2))
plot(factor3$loadings[,1]~pca$rotation[,1],xlab='PCA Loading 1',ylab='FA loading 1')
plot(factor3$loadings[,2]~pca$rotation[,2],xlab='PCA Loading 2',ylab='FA loading 2')
```


4. Comment on the observed variable specific variances (the uniquenesses). Do you believe all observed variables are explained by the factors discovered.  
```{r}
factor3$uniquenesses
```

CONT has a really high uniqueness, which means this variable does not fit neatly into the factors, whereas CFMG, FAMI, ORAL, WRIT have very low uniqueness values.  

5. Re-fit the 3 factor solution with a varimax rotation. How does this change the interpretation of the factors? In this case, do you find the rotated or non-rotated solution easier to interpret. Explain why or why not?
```{r}
factor3_rotate <- factanal(dataset_std, factors = 3, rotation = "varimax", 
                           scores = "regression", control=list(nstart=100))
factor3_rotate

plot(loadings(factor3_rotate))
text(factor3_rotate$loadings[,1], 
     factor3_rotate$loadings[,2],
      colnames(dataset_std),
      col="blue")
abline(h = 0, v = 0)
```
From the plot, we can clearly see different clusters of the variables, i.e., CONT has its own cluster. ORAL, CFMG, PREP, FAMI, WRIT, DILG, and DECI should be grouped together, and the rest 4 variables can be grouped together. Therefore, in this case, with a varimax rotation, it is easier to interpret.
